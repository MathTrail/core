ТЗ для Claude: Внедрение Apache Flink & CDC Pipeline
Контекст
Мы строим реактивную инфраструктуру для MathTrail. Основной поток данных идет через CDC из Postgres в Kafka. Apache Flink выступает в роли "умного фильтра и обогатителя", который на лету объединяет события (например, task_attempt) с данными профиля студента.

1. Инфраструктура и GitOps (Helm)
Helm-чарты: Скачать flink-kubernetes-operator и kafka-ui и Debezium Operator в mathtrail-charts/.

Kafka UI: Добавить в стек kafka-ui, настроив его на работу с твоим Kafka-кластером и Apicurio Schema Registry (чтобы он мог десериализовать Avro/Protobuf сообщения).

2. Сквозная архитектура (Документация)
Создать docs/architecture/stream-processing.md с Mermaid-диаграммой:

Source: Microservice (Writer) -> Postgres (WAL).

Ingress: Debezium (Postgres Connector) -> Kafka Source Topics.

Governance: Apicurio Registry (хранение схем).

Processing: Flink Job (RocksDB State) -> Обогащение данными из справочников.

Egress: Kafka Enriched Topics -> Consumers (Dapr-based microservices).

State Store: Persistent Volumes для RocksDB и Checkpoints.

3. Конфигурация FlinkDeployment (RocksDB Setup)
Подготовь YAML-манифест FlinkDeployment:

JobManager/TaskManager: Лимиты ресурсов (начни с 1 CPU / 2GB RAM для TM).

Flink Configuration:

state.backend: rocksdb

state.backend.incremental: true (для эффективных чекпоинтов).

execution.checkpointing.mode: EXACTLY_ONCE.

execution.checkpointing.interval: 1min.

Storage: Настройка volumeMounts для /opt/flink/volume/flink-checkpoints и данных RocksDB на быстрых дисках (Local Path Provisioner в k3s).

4. Observability & OpenTelemetry
Настрой "прозрачность" обработки:

Metrics: Настройка org.apache.flink.metrics.otel.OpenTelemetryMetricReporter. Проброс метрик лага (consumer lag) и времени обработки (latency) в OTel Collector.

Tracing: Реализация логики извлечения traceparent из заголовков Kafka сообщений внутри Flink Job (через ProcessFunction или SQL Metadata), чтобы связать логи записи в БД и логи обработки во Flink.

Logging: Логи в JSON через Log4j2 для Grafana Loki.

5. Потоковая логика (Flink SQL)
Подготовь пример SQL-запроса для Flink SQL Client/Gateway:

DDL: Создание таблиц с connector = 'upsert-kafka' или kafka.

Format: Использование avro-confluent с указанием URL твоего Apicurio Registry.

Query:

SQL
INSERT INTO enriched_events
SELECT e.*, p.skill_level, p.learning_style
FROM source_events e
LEFT JOIN profile_lookup FOR SYSTEM_TIME AS OF e.event_time AS p
ON e.student_id = p.id;